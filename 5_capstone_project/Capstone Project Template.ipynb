{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze I94 Immigration Data and the U.S. City Demographic Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project we we analyze the immigration dataset I94 Immigration Data and the U.S. City Demographic Data. Based on these data we could try to discover correlations between immigration and total population in certain states. Additionaly it would be possible to discover how much the immigration influences the foreign born statistic or do certain states with higher foreign born values attract more immigration. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import logging\n",
    "import os\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "For this project Apache Spark will be used for analyzing the datasets and preparing the data. Following datasets will be used: I94 Immigration Data and the U.S. City Demographic Data\n",
    "As the immigration dataset is very large we will use the data for APRIL 2016, this dataset allown contains over 3 million rows so this should be enough for fulfilling the project scope regrading the number of recrods. To additionaly enrich the data the demogprahic data is loaded to be able to analyze correlations bettwen these datasets. \n",
    "\n",
    "The data is loaed into datasets and cleaned of NaN values or empty rows. Additional dimension tables are generated out of the immigration data and demographic data. In the jupyter notebook we will first analyze the data and try discover best ways how to clean the data and how to setup the datamodel. The focus will be on local execution but the path variables in the configuration capstone file `capstone.cfd` is also prepared that we could execute the pipeline in the cloud with EMR. The ETL pipeline file is `etl.py `\n",
    "\n",
    "\n",
    "\n",
    "##### I94 Immigration Data Description\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.\n",
    "Data is locally stored in the workspace. This dataset is stored seperatley per month in the SAS format. We will use as test only the first data file for the APRIL `i94_apr16_sub.sas7bdat`. The path to the dataset is stored in the configuration file `capstone.cfg` under the keys `['DATASETS']['IMMIGRATION_DATA_PATH_LOCAL']`. To be a valid dataset for the capstone project it needs to contain at least 1 milion rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: datasets/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Number of rows: 3096313\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('capstone.cfg'))\n",
    "imigration_file_path= config['DATASETS']['IMMIGRATION_DATA_PATH_LOCAL']\n",
    "imigration_data_files=config['DATASETS']['IMMIGRATION_DATA_FILE'] \n",
    "imigration_path_and_file = os.path.join(imigration_file_path, imigration_data_files)\n",
    "print('File path: {}'.format(imigration_path_and_file))\n",
    "\n",
    "#Load imigration data over spark, downloading of teh package is not working so we will work with pandas dataframes\n",
    "#spark = SparkSession.builder.\\\n",
    "#config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "#.enableHiveSupport().getOrCreate()\n",
    "#df =spark.read.format('com.github.saurfang.sas.spark').load(imigration_path_and_file)\n",
    "\n",
    "# Load imigration dataset into pandas\n",
    "df = pd.read_sas(imigration_path_and_file, format='sas7bdat', encoding='ISO-8859-1')\n",
    "\n",
    "count_all_imigration_records = len(df)\n",
    "print('Number of rows: {}'.format(count_all_imigration_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of imigration records: 3096313\n",
      "Total number of columns: 28\n"
     ]
    }
   ],
   "source": [
    "# Validate if dataset has more then 1 million rows\n",
    "assert count_all_imigration_records > 1000000, 'Dataset must have more the 1 million rows'\n",
    "print('Total number of imigration records: {}'.format(count_all_imigration_records))\n",
    "num_of_columns_before_cleanup = df.shape[1]\n",
    "print('Total number of columns: {}'.format(num_of_columns_before_cleanup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247104e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247140e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "5   18.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MI   \n",
       "6   19.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NJ   \n",
       "\n",
       "   depdate  ...  entdepu  matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0      NaN  ...        U      NaN   1979.0  10282016    NaN    NaN     NaN   \n",
       "1      NaN  ...        Y      NaN   1991.0       D/S      M    NaN     NaN   \n",
       "2  20691.0  ...      NaN        M   1961.0  09302016      M    NaN      OS   \n",
       "3  20567.0  ...      NaN        M   1988.0  09302016    NaN    NaN      AA   \n",
       "4  20567.0  ...      NaN        M   2012.0  09302016    NaN    NaN      AA   \n",
       "5  20555.0  ...      NaN        M   1959.0  09302016    NaN    NaN      AZ   \n",
       "6  20558.0  ...      NaN        M   1953.0  09302016    NaN    NaN      AZ   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  1.897628e+09    NaN       B2  \n",
       "1  3.736796e+09  00296       F1  \n",
       "2  6.666432e+08     93       B2  \n",
       "3  9.246846e+10  00199       B2  \n",
       "4  9.246846e+10  00199       B2  \n",
       "5  9.247104e+10  00602       B1  \n",
       "6  9.247140e+10  00602       B2  \n",
       "\n",
       "[7 rows x 28 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset structure\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid       float64\n",
       "i94yr       float64\n",
       "i94mon      float64\n",
       "i94cit      float64\n",
       "i94res      float64\n",
       "i94port      object\n",
       "arrdate     float64\n",
       "i94mode     float64\n",
       "i94addr      object\n",
       "depdate     float64\n",
       "i94bir      float64\n",
       "i94visa     float64\n",
       "count       float64\n",
       "dtadfile     object\n",
       "visapost     object\n",
       "occup        object\n",
       "entdepa      object\n",
       "entdepd      object\n",
       "entdepu      object\n",
       "matflag      object\n",
       "biryear     float64\n",
       "dtaddto      object\n",
       "gender       object\n",
       "insnum       object\n",
       "airline      object\n",
       "admnum      float64\n",
       "fltno        object\n",
       "visatype     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print data types of imigration database\n",
    "# Details descirption is found in SAS file in dataset directory\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: datasets/us-cities-demographics.csv\n",
      "Number of demographic rows: 2891\n"
     ]
    }
   ],
   "source": [
    "config.read_file(open('capstone.cfg'))\n",
    "demographic_file_path= config['DATASETS']['DEMOGRAPHIC_DATA_PATH_LOCAL']\n",
    "demographic_file_name=config['DATASETS']['DEMOGRAPHIC_DATA_FILE'] \n",
    "demographic_path_and_file = os.path.join(demographic_file_path, demographic_file_name)\n",
    "print('File path: {}'.format(demographic_path_and_file))\n",
    "\n",
    "df_demographic = pd.read_csv(demographic_path_and_file,sep=';', header=0)\n",
    "count_all_demographic_recrods = len(df_demographic)\n",
    "print('Number of demographic rows: {}'.format(count_all_demographic_recrods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       object\n",
       "State                      object\n",
       "Median Age                float64\n",
       "Male Population           float64\n",
       "Female Population         float64\n",
       "Total Population            int64\n",
       "Number of Veterans        float64\n",
       "Foreign-born              float64\n",
       "Average Household Size    float64\n",
       "State Code                 object\n",
       "Race                       object\n",
       "Count                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print data types of demographic\n",
    "df_demographic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore of I94 Immigration Data \n",
    "We will frist try to find the columns containing most of the missing values. As a threshold we will define if a column has more the 70% missing values it will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entdepu</th>\n",
       "      <td>entdepu</td>\n",
       "      <td>99.987340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occup</th>\n",
       "      <td>occup</td>\n",
       "      <td>99.737559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insnum</th>\n",
       "      <td>insnum</td>\n",
       "      <td>96.327632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        column_name  percent_missing\n",
       "entdepu     entdepu        99.987340\n",
       "occup         occup        99.737559\n",
       "insnum       insnum        96.327632"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "\n",
    "missing_value_df.sort_values('percent_missing', ascending=False, inplace=True)\n",
    "\n",
    "# Show all columns that have more the\n",
    "missing_value_df_above_70 = missing_value_df[missing_value_df.percent_missing > 70]\n",
    "\n",
    "missing_value_df_above_70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaunup of I94 Immigration Data \n",
    "For the immigration data we will remove every column that has more the 70% missing values. Also we will remove any duplicates from the primary keys and drop all rows containing NaN values as primary key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after cleanup: 25\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = df.drop(columns=missing_value_df_above_70['column_name'].tolist())\n",
    "num_columns_after_cleanup = cleaned_df.shape[1]\n",
    "print('Number of columns after cleanup: {}'.format(num_columns_after_cleanup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to cleanup all duplicates and remove keys with NAN values for the cicid primary key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleanup: 3096313\n",
      "Number of rows after cleanup: 3096313\n",
      "Number of columns before cleanup: 28\n",
      "Number of columns after cleanup: 25\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.drop_duplicates(subset='cicid')\n",
    "cleaned_df = cleaned_df.dropna(subset = ['cicid'])\n",
    "\n",
    "print('Number of rows before cleanup: {}'.format(count_all_imigration_records))\n",
    "print('Number of rows after cleanup: {}'.format(len(cleaned_df)))\n",
    "print('Number of columns before cleanup: {}'.format(num_of_columns_before_cleanup))\n",
    "print('Number of columns after cleanup: {}'.format(num_columns_after_cleanup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore of U.S. City Demographic Data\n",
    "\n",
    "We will frist try to find the columns containing most of the missing values. As a threshold we will define if a column has more the 70% missing values it will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average Household Size</th>\n",
       "      <td>Average Household Size</td>\n",
       "      <td>0.553442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Veterans</th>\n",
       "      <td>Number of Veterans</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign-born</th>\n",
       "      <td>Foreign-born</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male Population</th>\n",
       "      <td>Male Population</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female Population</th>\n",
       "      <td>Female Population</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   column_name  percent_missing\n",
       "Average Household Size  Average Household Size         0.553442\n",
       "Number of Veterans          Number of Veterans         0.449671\n",
       "Foreign-born                      Foreign-born         0.449671\n",
       "Male Population                Male Population         0.103770\n",
       "Female Population            Female Population         0.103770"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing_dem = df_demographic.isnull().sum() * 100 / len(df_demographic)\n",
    "\n",
    "missing_value_df_dem = pd.DataFrame({'column_name': df_demographic.columns,\n",
    "                                 'percent_missing': percent_missing_dem})\n",
    "\n",
    "missing_value_df_dem.sort_values('percent_missing', ascending=False, inplace=True)\n",
    "\n",
    "missing_value_df_above_01_dem = missing_value_df_dem[missing_value_df_dem.percent_missing > 0.1]\n",
    "missing_value_df_above_01_dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't have a significat NaN values in certain collums we will continue with searching other data inconsistency. \n",
    "We will continues do check if there are duplicate values or Nan values. As pimary key mutliple columns must be used: `'City','Race','State Code'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleanup: 2891\n",
      "Number of rows after cleanup: 2875\n"
     ]
    }
   ],
   "source": [
    "clean_df_demographic = df_demographic.drop_duplicates(subset=['City','Race','State Code'])\n",
    "clean_df_demographic = clean_df_demographic.dropna()\n",
    "\n",
    "print('Number of rows before cleanup: {}'.format(count_all_demographic_recrods))\n",
    "print('Number of rows after cleanup: {}'.format(len(clean_df_demographic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "![Datamodel](img/data_model_3.png)\n",
    "\n",
    "The fact table comes from the immigration dataset. This dataset in his raw setup contains 28 columns. With addtitonal cleanup of columns which has NaN values the fact datamode is reduce to 25 columns. The primary key is the cicid column which we rename to *id*. \n",
    "\n",
    "The dimensions tables are generated from the immigration dataset values and additionaly from the demographics dataset.\n",
    "The *dim_port_code* dimension tables is generated from the descprition of the immigration dataset. The port codes are saved in a seperated csv files under [port_codes](datasets/port_codes.csv). \n",
    "\n",
    "The *dim_arr_date* table is created from the arrival date of the immigration dataset. Additionaly the data is enriched with month, weekday, day, hour, year data from the arrival date. The *dim_visa_type* table contains visa_types from the immigration datasets. The *dim_gemographics* table contains data from the demographics datasets about based on the US state level. The link to the immigration dataset is the arrival state in the immigration fact table. \n",
    "\n",
    "Based on the correltions between the fact and demographics dimension we could try to discover possible relations betwwen states with high population and immigration arrival. Additional relations between foreign born values in certain stats or houshold sizes could be disovered and immigration arrivals in certain states.  \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Lets first create the dimenstion tables out of the cleanded datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Analyze content of  visatype dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GMB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0    B2\n",
       "1    F1\n",
       "2    B1\n",
       "3    WT\n",
       "4    WB\n",
       "5    E2\n",
       "6     I\n",
       "7    F2\n",
       "8    E1\n",
       "9    M1\n",
       "10   I1\n",
       "11   CP\n",
       "12   M2\n",
       "13  CPL\n",
       "14  SBP\n",
       "15  GMT\n",
       "16  GMB"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get contetn of dimenstion visatype table\n",
    "distinct_visatype = df['visatype'].unique()\n",
    "visatype_dimensions = pd.DataFrame(distinct_visatype)\n",
    "visatype_dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Analyze port dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path for port data: datasets/port_codes.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKERAAF-BAKERISLAND</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONSCACHE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEWSTATIONPTLAYDEW</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCHHARBOR</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EGL</td>\n",
       "      <td>EAGLE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRB</td>\n",
       "      <td>FAIRBANKS</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOM</td>\n",
       "      <td>HOMER</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HYD</td>\n",
       "      <td>HYDER</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port                  city state\n",
       "0  ALC                 ALCAN    AK\n",
       "1  ANC             ANCHORAGE    AK\n",
       "2  BAR  BAKERAAF-BAKERISLAND    AK\n",
       "3  DAC          DALTONSCACHE    AK\n",
       "4  PIZ    DEWSTATIONPTLAYDEW    AK\n",
       "5  DTH           DUTCHHARBOR    AK\n",
       "6  EGL                 EAGLE    AK\n",
       "7  FRB             FAIRBANKS    AK\n",
       "8  HOM                 HOMER    AK\n",
       "9  HYD                 HYDER    AK"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.read_file(open('capstone.cfg'))\n",
    "port_file_path= config['DATASETS']['PORT_DATA_PATH_LOCAL']\n",
    "port_file_name=config['DATASETS']['PORT_DATA_DATA_FILE'] \n",
    "port_path_and_file = os.path.join(port_file_path, port_file_name)\n",
    "print('File path for port data: {}'.format(port_path_and_file))\n",
    "df_port = pd.read_csv(port_path_and_file,sep=';', header=0)\n",
    "df_port.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Analyze arravial date table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arr_date_table(df):\n",
    "    \"\"\"\n",
    "    This method creates arrival date table from immigration data frame as input\n",
    "    :param df: dataframe of immigration data\n",
    "    :return: arr_date table with arrival date broken down to single values\n",
    "    \"\"\"\n",
    "\n",
    "    # SAS date value is a value that represents the number of days between January 1, 1960, and a specified\n",
    "    # This UDF converts SAS data value to standard datetime format\n",
    "    @udf(StringType())\n",
    "    def convert_sas_to_datetime(x):\n",
    "        if x:\n",
    "            return datetime(1960, 1, 1).date() + dt.timedelta(x).isoformat()\n",
    "        return None\n",
    "\n",
    "    arr_date_table = df.select(['arrdate']).withColumn(\"arr_date\", convert_sas_to_datetime(df.arrdate)).distinct()\n",
    "    arr_date_table = arr_date_table.withColumn('arrival_day', dayofmonth('arr_date'))\n",
    "    arr_date_table = arr_date_table.withColumn('arrival_week', weekofyear('arr_date'))\n",
    "    arr_date_table = arr_date_table.withColumn('arrival_month', month('arr_date'))\n",
    "    arr_date_table = arr_date_table.withColumn('arrival_year', year('arr_date'))\n",
    "    arr_date_table = arr_date_table.withColumn('arrival_weekday', dayofweek('arr_date'))\n",
    "    arr_date_table = arr_date_table.withColumn('id', monotonically_increasing_id())\n",
    "    return arr_date_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "The immigration datasets and dimension datasets are validate if the contain any empty ids. Additionaly the generated dimension tables visa_type and arr_date are validate if they the same number of distinct records like the fact tables.\n",
    " \n",
    "Example quality checks of immigration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def data_quality_check_immigration(spark, immigration_fact, dimension_port_code, dimension_arr_date,\n",
    "                                   dimension_visa_type):\n",
    "    \"\"\"\n",
    "    Method checks if all tables dont contain null values in primary key. Also dimension table check are executed\n",
    "    :param spark: spark session context\n",
    "    :param immigration_fact: immigration dataframe\n",
    "    :param dimension_port_code:  port_code dataframe\n",
    "    :param dimension_arr_date:  arr_date dimension dataframe\n",
    "    :param dimension_visa_type: visa_type dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_check = [\n",
    "        ('SELECT COUNT(*) FROM immigration_fact WHERE id IS NULL', 0),\n",
    "        ('SELECT COUNT(*) FROM dimension_port_code WHERE port_code IS NULL', 0),\n",
    "        ('SELECT COUNT(*) FROM dimension_arr_date WHERE arr_date IS NULL', 0),\n",
    "        ('SELECT COUNT(*) FROM dimension_visa_type WHERE visa_type IS NULL', 0)\n",
    "    ]\n",
    "    for query in sql_check:\n",
    "        print('--------------------EXECUTING SQL TEST-------------------------')\n",
    "        sql_query, expected_result = query\n",
    "        print(\"Validating query:{} , Expecting result {}\".format(sql_query, expected_result))\n",
    "        result = spark.sql(sql_query)\n",
    "        new_count = result.collect()[0][0]\n",
    "        if new_count != expected_result:\n",
    "            raise ValueError(\"ERROR Validating query:{} , Expected result: {}, Returned_result: {}\".format(sql_query,\n",
    "                                                                                                           expected_result,\n",
    "                                                                                                           new_count))\n",
    "        else:\n",
    "            print(\"Validation OK! Validating query:{} , Expected result: {}, Returned_result: {}\".format(sql_query,\n",
    "                                                                                                         expected_result,\n",
    "                                                                                                         new_count))\n",
    "\n",
    "    distinct_arr_date_fact = immigration_fact.select(\"arr_date\").distinct().count()\n",
    "    distinct_arr_date_dimension = dimension_port_code.distinct().count()\n",
    "    if distinct_arr_date_fact != distinct_arr_date_dimension:\n",
    "        ValueError(\n",
    "            \"arr_date in fact table {} are not equal arr_date in dimension table {}\".format(distinct_arr_date_fact,\n",
    "                                                                                            distinct_arr_date_dimension))\n",
    "\n",
    "    distinct_visa_type_fact = immigration_fact.select(\"visa_type_id\").distinct().count()\n",
    "    distinct_visa_type_dimension = dimension_visa_type.distinct().count()\n",
    "    if distinct_visa_type_fact != distinct_visa_type_dimension:\n",
    "        ValueError(\n",
    "            \"visa_type in fact table {} are not equal visa_type in dimension table {}\".format(distinct_visa_type_fact,\n",
    "                                                                                              distinct_visa_type_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "<center><b> fact_imigration</cetner>  <b>\n",
    "<br/>\n",
    "    \n",
    "| Column        | Description   |\n",
    "| :------------- |:--------------|\n",
    "| id            | created from cicid used from as primary key |\n",
    "| arr_date      | arrivale date in the USA.      |\n",
    "| arr_mode      | arrival mode origin field is i94mode      |\n",
    "| dep_date      | departure date       |\n",
    "| age      | age at arrivale from I94BIR column     |\n",
    "| visa_code      | I94VISA - Visa codes collapsed into three categories:1 = Business, 2 = Pleasure,  3 = Student |\n",
    "| port_code      | port code from i94port      |  \n",
    "| visa_post      | Department of State where where Visa was issued     |    \n",
    "| gender         | Gender    |      \n",
    "| airline        | Airline used to arrive in U.S.     |    \n",
    "| visa_post      | Department of State where where Visa was issued     |    \n",
    "| flight_num     | Flight number    |  \n",
    "| visa_type_id   | Foreign key of visa type dimension table    |  \n",
    "| birth_country_code   | code of birth country   |  \n",
    "| residence_code   | residence code from I94RES    |  \n",
    "| match_fag   | Match of arrival and departure records    |  \n",
    "| birth_year   | birth year   | \n",
    "| stay_until   | Date to which admitted to U.S. (allowed to stay until)   | \n",
    "| adm_num   | Admission Number   | \n",
    "| arr_flag   | Arrival Flag - admitted or paroled into the U.S | \n",
    "| dep_flag   | Departure Flag - Departed, lost I-94 or is deceased   | \n",
    "|dtadfile|Character Date Field - Date added to I-94 Files|\n",
    "|year |4 digit year |\n",
    "|month|Numeric month |\n",
    "|state|Address state|\n",
    "    \n",
    "<br/><br/>\n",
    "\n",
    " <center> <b> dim_port_code <b></center>\n",
    "\n",
    "| Column        | Description   |\n",
    "| :------------- |:-------------|\n",
    "| port_code            | airport code |\n",
    "| city      | airport city      |\n",
    "| state      | airport state|\n",
    "        \n",
    "     \n",
    "<br/><br/>\n",
    "<center> <b> dim_demographics <b></center>\n",
    "        \n",
    "| Column        | Description   |\n",
    "| :-------------- |:-------------|\n",
    "| median_age            | city median age |\n",
    "| male_population      | male population of the city |\n",
    "| number_of_veterans      | number of veterane|\n",
    "| female_population            | female population |\n",
    "| total_population      | total population in city   |\n",
    "| state_code      | city state code|   \n",
    "| average_household_size      | average household size|   \n",
    "| foreign_born      | foreign born in city|   \n",
    "| city| city name | \n",
    "| state_name| state name | \n",
    "| state_code| state code| \n",
    "| race|  race name | \n",
    "| count| population number in certain race | \n",
    "\n",
    "<br/><br/>\n",
    "<center> <b> dim_visa_type <b></center>\n",
    "    \n",
    "| Column        | Description   |\n",
    "| :------------- |:-------------|\n",
    "| visa_type_id            | unique id of visa type|\n",
    "| visa_type      | Class of admission legally admitting the non-immigrant to temporarily stay    |\n",
    "\n",
    "<br/><br/>\n",
    "<center> <b> dim_arr_date <b></center>\n",
    "\n",
    "| Column        | Description   |\n",
    "| :------------- |:-------------|\n",
    "| arrdate            | arrival date |\n",
    "| arr_day      | arrival day      |\n",
    "| arr_week      | arrival week|\n",
    "| arr_month      | arrival month|    \n",
    "| arr_year      | arrival year|\n",
    "| arr_weekday      | arrival weekday|    \n",
    "| id      | unique key|  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Following tehnologies has been used for the project:\n",
    "\n",
    "- Python - Python is easy to learn as well because of its simple syntax. This simple, readable syntax helps Big Data pros to focus on insights managing Big data, rather than wasting time in understanding technical nuances of the language. This one is one of the primary reasons to choose Python for Big Data.\n",
    "\n",
    "- Apache Pysport - Provides several advantages over MapReduce: it is faster, easier to use, offers simplicity, and runs virtually everywhere. It has built-in tools for SQL, Machine Learning, and streaming which make it a very popular and one of the most asked tools in the IT industry.\n",
    "\n",
    "- Pandas -  is mainly used for data analysis and for analyzing the immigration and demographic datasets. Pandas allows importing data from various file formats such as comma-separated values, JSON, SQL, Microsoft Excel. Pandas allows various data manipulation operations such as merging, reshaping, selecting, as well as data cleaning, and data wrangling features.\n",
    "\n",
    "Data can be update on monthly bases as new datasets come out trends can be analyzed on month based especially for the immigration fact table which is partitioned by month. \n",
    "\n",
    "##### Questions:\n",
    "\n",
    "*The data was increased by 100x.*\n",
    "- Data can be partitioned by lower granularity probably by weekly or daily basis depending on the growth of the data. Additional EMR can be used for such cases adding worker not with more compute power and memory.\n",
    "    \n",
    "\n",
    "*If the pipelines were run on a daily basis by 7am*\n",
    "- Apache Airflow can be used for scheduling workflows \n",
    "\n",
    "*If the database needed to be accessed by 100+ people*\n",
    "- Amazon Redshfit can be used if the data needs cerain access priviliges. Additionaly adding more nodes to Redshift or bigger cluster will help us to deal with the necessary write scaling \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}